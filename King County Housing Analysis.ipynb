{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51982b31-4450-4239-8d0e-e2f17077c2c8",
   "metadata": {},
   "source": [
    "# Predicting King County Housing Prices: A Data-Driven Approach to Feature Correlation Analysis and Machine Learning Model Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c72c5a-5e85-4764-8d05-995ddd6bb981",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ad7d9b-5895-4a32-b26c-96bcee5ea721",
   "metadata": {},
   "source": [
    "Housing prices in King County are influenced by a variety of factors, but which characteristics have the strongest impact, and do these trends differ across neighborhoods? This analysis seeks to answer the question: How do key housing features—such as square footage, renovation status, waterfront location, and proximity to urban centers—affect sale prices, and do these relationships vary by neighborhood? By examining these factors, we can uncover whether buyer preferences prioritize space, modern updates, scenic locations, or urban accessibility—and whether these priorities shift depending on the area.\n",
    "\n",
    "This question is important because it goes beyond general price trends and explores what truly drives value in different parts of King County. Real estate agents, urban planners, and potential buyers would all benefit from understanding these patterns. For example, if renovated homes command a higher premium in suburban areas but not in downtown Seattle, this could reflect differing buyer expectations. Similarly, if waterfront properties consistently sell for more regardless of neighborhood, it may indicate a broader demand for luxury amenities. These insights could also inform discussions about housing affordability, development priorities, and economic disparities across the region.\n",
    "\n",
    "To answer this question, we would analyze both quantitative and categorical variables. Key metrics would include sale price (the dependent variable), square footage, year built, and distance to major urban centers like Seattle (all quantitative). Categorical variables would include whether a home is waterfront (yes/no), has been renovated (yes/no), and its neighborhood classification. Geographic coordinates (latitude/longitude) would allow for spatial analysis to detect location-based trends."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04dc1a8-866d-4309-b992-be37c4858d75",
   "metadata": {},
   "source": [
    "#### Data Science Question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027f01ca-06d2-436f-a69a-7c0b2d95bc6b",
   "metadata": {},
   "source": [
    "How have housing market trends in King County evolved over time? What factors influence property values? Do these trends vary by location or property characteristics? What might these patterns indicate about shifting homeowner preferences or economic conditions?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80de8b32-7563-4e00-853b-d5892ad6ab70",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c16cc44-9f58-41bd-a10f-9a1c5914e5e2",
   "metadata": {},
   "source": [
    "This code imports essential libraries for data analysis, geospatial visualization, and predictive modeling. Pandas and NumPy enable efficient data manipulation and statistical calculations, while Geopandas and Folium facilitate mapping and spatial analysis to explore geographic patterns in property locations. Machine learning tools like Scikit-learn will be used to build regression models, identify key predictors of housing prices (e.g., square footage, waterfront status, or renovation year), and quantify their impact. Together, these tools allow us to visualize price distributions across neighborhoods, analyze historical trends, and uncover the most influential factors driving King County’s real estate market dynamics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b378cfe5-df25-4cbb-9c32-d6c2d7169528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Standard Libraries ==========\n",
    "import os\n",
    "import json\n",
    "import datetime\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ========== Geospatial Libraries ==========\n",
    "import folium\n",
    "from folium import plugins\n",
    "from folium.plugins import MarkerCluster\n",
    "from geopy.exc import GeocoderTimedOut, GeocoderServiceError\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "\n",
    "# ========== Plotly for Interactive Visualizations ==========\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# ========== Scikit-Learn Libraries ==========\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error,\n",
    "    r2_score,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    "    classification_report\n",
    ")\n",
    "\n",
    "# ========== Machine Learning Models ==========\n",
    "# Linear Models\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Ensemble Models\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "\n",
    "# Tree-Based Models\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "\n",
    "# Distance-Based Models\n",
    "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
    "\n",
    "# Naive Bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Support Vector Machines\n",
    "from sklearn.svm import SVR, SVC\n",
    "\n",
    "# Neural Networks\n",
    "from sklearn.neural_network import MLPRegressor, MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb504a4-a75e-43ef-9aa5-1dea3d719563",
   "metadata": {},
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7432fc62-3306-49c5-bbaa-c19c85f65ff6",
   "metadata": {},
   "source": [
    "Let's take a peek at the first few rows of the data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce2fd10-d785-4117-adc3-979278480bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file\n",
    "df = pd.read_csv('kc_house_data.csv')\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8e2ea1-7aea-4b25-8dbe-aecdcde02c6f",
   "metadata": {},
   "source": [
    "We notice that there is 21 different features (columns). The id, date, price, bedrooms, bathrooms, sqft_living, sqft_lot, floors, waterfront, view, condition, grade, sqft_above, sqft_basement, yr_built, yr_renovated, zipcode, lat, long, sqft_living15, sqft_lot15.\n",
    "\n",
    "Columns:\n",
    "- ida: notation for a house\n",
    "- date: The date the house was sold (ranges from May 2014 to May 2015)\n",
    "- price: Price is prediction target\n",
    "- bedrooms: Number of Bedrooms in the house\n",
    "- bathrooms: Number of bathrooms in the house\n",
    "- sqft_living: square footage of the home\n",
    "- sqft_lot: square footage of the lot\n",
    "- floors: Total floors (levels) in the house\n",
    "- waterfront: Determines if the house has a view to the waterfront; in other words is close tp water (Determined by 0 and 1 as 0 being no and 1 being yes)\n",
    "- view: Has been viewed (Determined between 0 and 4)\n",
    "- condition: How good the condition is overall (Determined between 0 and 4)\n",
    "- grade: The overall grade given to the housing unit (based on King County grading system)\n",
    "- sqft_abovesquare: footage of house subtracting the basement\n",
    "- sqft_basement: the square footage of the basement of the house (if the vaklue is 0 then there is no basement in the house)\n",
    "- yr_built: The year the house was built\n",
    "- yr_renovated: The year when the house was renovated\n",
    "- zipcode: zip\n",
    "- lat: Latitude coordinate\n",
    "- long: Longitude coordinate\n",
    "- sqft_living15: square footage of the home in 2015(implies-- some renovations)\n",
    "- sqft_lot15: square footage of the lot in 2015(implies-- some renovations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a138b448-292d-4f9a-a573-3c5307497cbe",
   "metadata": {},
   "source": [
    "### Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8018c2f4-a0e3-4339-b664-a51c5ab05ef1",
   "metadata": {},
   "source": [
    "The analysis covers over 20000 addresses spanning from 1900 to 2015 that were sold during May 2014-May 2015 in King County, USA\n",
    "\n",
    "Dataset was pulled from https://www.kaggle.com/datasets/soylevbeytullah/house-prices-dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b508c8d1-46ae-469e-9ce4-fb07c111901d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406a7456-dca4-4fb3-8dd3-f81a05353771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get summary statistics\n",
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10750993-11b9-4829-8fb9-1e8f3334b644",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c55263-0a45-4bcc-85d2-1cd3ed34c96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many columns and rows in the dataset\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238ed88c-9686-41fe-a62c-a136b67af5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of cells in the dataset\n",
    "df.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314efdf1-4e65-4a2d-b62b-7130407283da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking to see if there any duplicated values\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b135cc0-9dde-426f-aab4-e54df585e0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking to see if any data is null/empty\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9e4f17-ad4f-44de-9233-6c54df1a5b65",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3576641c-3607-49cc-854b-ec965aa73386",
   "metadata": {},
   "source": [
    "### Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63298c38-a98c-4fdf-97e3-a699b0765e5b",
   "metadata": {},
   "source": [
    "Once I had gotten the basic information of the dataset, there were columns I needed to add and fix.\n",
    "\n",
    "Firstly, I will add a description column that essentially groups all information of the property into one paragraph. Variable will be called home_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361741db-9143-491c-ac45-65cc54d0f120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a house description\n",
    "def create_property_description(row):\n",
    "    description = f\"This {row['bedrooms']} bedroom, {row['bathrooms']} bathroom home features \"\n",
    "    description += f\"{row['sqft_living']:,} sqft of living space on a {row['sqft_lot']:,} sqft lot. \"\n",
    "    description += f\"The property has {row['floors']} floor(s) and includes \"\n",
    "    \n",
    "    # Waterfront information\n",
    "    if row['waterfront'] == 1:\n",
    "        description += \"a waterfront view, \"\n",
    "    \n",
    "    # View quality\n",
    "    view_descriptions = {\n",
    "        0: \"no special views\",\n",
    "        1: \"a limited view\",\n",
    "        2: \"a decent view\",\n",
    "        3: \"a good view\",\n",
    "        4: \"an excellent view\"\n",
    "    }\n",
    "    description += f\"{view_descriptions.get(row['view'], 'a view')} and is in \"\n",
    "    \n",
    "    # Condition description\n",
    "    condition_descriptions = {\n",
    "        1: \"poor\",\n",
    "        2: \"fair\",\n",
    "        3: \"average\",\n",
    "        4: \"good\",\n",
    "        5: \"excellent\"\n",
    "    }\n",
    "    description += f\"{condition_descriptions.get(row['condition'], 'unknown')} condition. \"\n",
    "    \n",
    "    # Square footage details\n",
    "    description += f\"It has {row['sqft_above']:,} sqft above ground. \"\n",
    "    if row['sqft_basement'] > 0:\n",
    "        description += f\"and {row['sqft_basement']:,} sqft of basement space. \"\n",
    "    \n",
    "    # Year and renovation information\n",
    "    description += f\"Built in {row['yr_built']}\"\n",
    "    if row['yr_renovated'] > 0:\n",
    "        description += f\", renovated in {row['yr_renovated']}. \"\n",
    "        description += (f\"In 2015, the home had a living area of {row['sqft_living15']:,} sqft and \"\n",
    "                        f\"a lot size of {row['sqft_lot15']:,} sqft, compared to the current \"\n",
    "                        f\"{row['sqft_living']:,} sqft living space and {row['sqft_lot']:,} sqft lot. \")\n",
    "    else:\n",
    "        description += \". \"\n",
    "    \n",
    "    # Overall grade information\n",
    "    description += f\"It has an overall grade of {row['grade']}, according to the King County grading system.\"\n",
    "    \n",
    "    return description\n",
    "\n",
    "# Assuming your DataFrame is named 'df'\n",
    "df['home_description'] = df.apply(create_property_description, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cecc258-468c-471b-a9b8-72a3b5b3b374",
   "metadata": {},
   "source": [
    "Example property description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c1acd2-7d22-49ff-9848-cb21b776aecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a sample description from the first\n",
    "df['home_description'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2d3b26-d2e2-4a12-8165-bfe6d31e63f3",
   "metadata": {},
   "source": [
    "Then I fixed the date column as the date looked in this format \"20141013T000000\", the T000000 at the end of the date is not needed so I removed it. Also I formatted the date as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530c522d-5735-42f4-aed8-eb07fe4f038c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your DataFrame is named 'df' and the column with the date is named 'date'\n",
    "df['date'] = df['date'].str.replace('T000000', '')  # Remove 'T000000'\n",
    "df['date'] = pd.to_datetime(df['date'], format='%Y%m%d')  # Convert to datetime format\n",
    "df['date'] = df['date'].dt.strftime('%Y/%m/%d')  # Format as 'yyyy/mm/dd'\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706651d5-d302-4e37-9cdd-dde13460b892",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39149ff1-e5f6-40c2-8923-f45cab305c5e",
   "metadata": {},
   "source": [
    "This code creates the color map based on the price of the propterty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7a26ad-4f71-447f-a331-48f29f437f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the create_color_map function\n",
    "def create_color_map(color_coords, color_bounds):\n",
    "    def to_cmap_coord(x, level=0.0):\n",
    "        return (level, np.interp(x, xp=[0, 255], fp=[0, 1]), np.interp(x, xp=[0, 255], fp=[0, 1]))\n",
    "    \n",
    "    cmap_price_bounds = [np.interp(p, xp=[min(color_bounds), max(color_bounds)], fp=[0, 1]) for p in color_bounds]\n",
    "    \n",
    "    c_dict = {\n",
    "        'red': tuple(to_cmap_coord(color_coords[i][0], cmap_price_bounds[i]) for i in range(len(color_coords))),\n",
    "        'green': tuple(to_cmap_coord(color_coords[i][1], cmap_price_bounds[i]) for i in range(len(color_coords))),\n",
    "        'blue': tuple(to_cmap_coord(color_coords[i][2], cmap_price_bounds[i]) for i in range(len(color_coords))),\n",
    "    }\n",
    "\n",
    "    return matplotlib.colors.LinearSegmentedColormap('cmap', segmentdata=c_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b820ab-77da-4f9c-afc2-96d7abd5d811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Color Boundaries (in $s)\n",
    "cmap_price_bounds = [0, 800000, 1500000, 10000000]\n",
    "# Color Definitions (in RGB)\n",
    "color_coords = [\n",
    "    (47, 216, 58),    # Green ($0 - $800,000)\n",
    "    (215, 237, 23),   # Yellow ($800,000 - $1,500,000)\n",
    "    (239, 32, 21),    # Red ($1,500,000 - $10,000,000)\n",
    "    (117, 11, 5)      # Darker Red (> $10,000,000)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0103997-5994-4fe1-b62a-a5535b3332cd",
   "metadata": {},
   "source": [
    "Finally, I used lat and long to find the addresses which was the new column as well to add. Variable will be called address.\n",
    "\n",
    "This is where I find the addresses of every location based on the lat and long. Then the address found will be placed in the table.\n",
    "\n",
    "Originally I had it that this ran everytime I clicked it, issue was due to the large size of the dataset, it took 8 hours to get all addresses, with improvements I instead added the addresses found in a cache file so its faster to get and retrieve them without doing the whole process again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620f171d-9ffc-4f4b-9bb3-7fe4caaa4299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Nominatim API\n",
    "geolocator = Nominatim(user_agent=\"real_estate_map\")\n",
    "\n",
    "# Add rate limiting to avoid hitting API limits\n",
    "reverse = RateLimiter(geolocator.reverse, min_delay_seconds=1)\n",
    "\n",
    "# Define a function to get the address from latitude and longitude\n",
    "def get_address(lat, lon):\n",
    "    location = reverse((lat, lon), exactly_one=True)\n",
    "    return location.address if location else \"Address not found\"\n",
    "\n",
    "# Cache file path\n",
    "cache_file = \"address_cache.json\"\n",
    "\n",
    "# Load existing cache if it exists\n",
    "if os.path.exists(cache_file):\n",
    "    with open(cache_file, \"r\") as f:\n",
    "        address_cache = json.load(f)\n",
    "else:\n",
    "    address_cache = {}\n",
    "\n",
    "# Function to fetch address with caching\n",
    "def get_cached_address(lat, lon):\n",
    "    # Create a unique key for the coordinates\n",
    "    key = f\"{lat},{lon}\"\n",
    "    \n",
    "    # Check if the address is already in the cache\n",
    "    if key in address_cache:\n",
    "        return address_cache[key]\n",
    "    \n",
    "    # If not in cache, fetch the address and save it\n",
    "    address = get_address(lat, lon)\n",
    "    address_cache[key] = address\n",
    "    \n",
    "    # Save the updated cache to the file\n",
    "    with open(cache_file, \"w\") as f:\n",
    "        json.dump(address_cache, f)\n",
    "    \n",
    "    return address\n",
    "\n",
    "# Use tqdm to show progress while fetching addresses\n",
    "tqdm.pandas(desc=\"Fetching addresses\")  # Initialize tqdm for pandas\n",
    "\n",
    "# Add a new column 'address' to the original DataFrame df\n",
    "df['address'] = df.progress_apply(lambda row: get_cached_address(row['lat'], row['long']), axis=1)\n",
    "\n",
    "# Display the first few rows of the updated DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc58008-a711-4d74-9daf-f9ac659e242e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the colormap\n",
    "cmap = create_color_map(color_coords, cmap_price_bounds)\n",
    "\n",
    "# Create the folium map instance\n",
    "real_estate_map = folium.Map(location=[47.61038000, -122.20068000], zoom_start=12)\n",
    "\n",
    "# Define the cmap_func function\n",
    "def cmap_func(row, cmap):\n",
    "    r_interp = np.interp(row['price'], xp=[df['price'].min(), df['price'].max()], fp=[5, 20])\n",
    "    c_interp = np.interp(row['price'], xp=[df['price'].min(), df['price'].max()], fp=[0, 1])\n",
    "    o_interp = np.interp(row['price'], xp=[df['price'].min(), df['price'].max()], fp=[0.2, 1.0])\n",
    "    inner_color = matplotlib.colors.to_hex(list(cmap(c_interp))[:3] + [o_interp])\n",
    "    \n",
    "    popup_text = folium.Popup(\n",
    "    \"Price: \" + '${:,d}'.format(int(row['price'])) +\n",
    "    \"<br>Address: \" + row['address'] +\n",
    "    \"<br>Description: \" + row['home_description'], \n",
    "    max_width=450\n",
    ")\n",
    "\n",
    "    \n",
    "    folium.CircleMarker(location=[row['lat'], row['long']],\n",
    "                        radius=r_interp, weight=0.9, color='black', fill_color=inner_color,\n",
    "                        popup=popup_text).add_to(real_estate_map)\n",
    "\n",
    "# Apply the cmap_func to each row in the DataFrame\n",
    "df.apply(lambda row: cmap_func(row, cmap), axis=1)\n",
    "\n",
    "# Display the map\n",
    "real_estate_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6620b806-566e-4610-933c-8568490999c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define price intervals and labels for the pie chart\n",
    "labels = ['$0 - $250K', '$250K - $400K', '$400K - $600K', '$600K - $800K', '$800K - $1.5M', 'More than $1.5M']\n",
    "colors = ['#13af2a', '#10d32e', '#dbf70c', '#f4ac04', '#e86914', '#f41313']\n",
    "intervals = [0, 250000, 400000, 600000, 800000, 1500000, df['price'].max()]  # Price intervals in $s\n",
    "\n",
    "# Calculate the size of each pie chart slice\n",
    "chart_slice_sizes = df.groupby(pd.cut(df['price'], intervals)).size().values\n",
    "\n",
    "# Calculate median price for annotation\n",
    "median_price = df['price'].median()\n",
    "\n",
    "# Create the parent figure\n",
    "fig, ax = plt.subplots(2, 2)\n",
    "fig.set_size_inches(15, 12)\n",
    "fig.suptitle('Home Prices Analysis', size=21)\n",
    "\n",
    "# Create pie chart\n",
    "ax[0][0].set_title('Pie Chart', size=20, pad=20)\n",
    "ax[0][0].pie(chart_slice_sizes, labels=labels, colors=colors, startangle=30, autopct='%1.1f%%',\n",
    "             wedgeprops={'edgecolor': 'black'})\n",
    "\n",
    "# Create histogram\n",
    "ax[0][1].set_title('Histogram', size=20, pad=20)\n",
    "ax[0][1].hist(df['price'], bins=280, color='forestgreen')\n",
    "ax[0][1].set_ylabel('# Homes', size=18)\n",
    "ax[0][1].set_xlabel('Price in $s (log scaled)', size=18)\n",
    "ax[0][1].set_xscale('log')\n",
    "median_line = ax[0][1].axvline(median_price, color='lightcoral', label=f'Median: ${median_price:,.0f}', linewidth=2)\n",
    "ax[0][1].legend()\n",
    "\n",
    "# Create Scatterplot\n",
    "ax[1][0].set_title('Scatter Plot', size=20, pad=20)\n",
    "ax[1][0].scatter(range(len(df)), df['price'], marker='.', color='dodgerblue')\n",
    "ax[1][0].set_ylabel('Price in $s (log scaled)', size=18)\n",
    "ax[1][0].set_xlabel('Homes', size=18)\n",
    "ax[1][0].set_yscale('log')\n",
    "median_line = ax[1][0].axhline(median_price, color='lightcoral', label=f'Median: ${median_price:,.0f}', linewidth=2)\n",
    "ax[1][0].legend()\n",
    "\n",
    "# Create Boxplot\n",
    "ax[1][1].set_title('Box Plot', size=20, pad=20)\n",
    "ax[1][1].boxplot(df['price'], sym='r.', vert=False, showmeans=True, meanline=True)\n",
    "ax[1][1].set_xlabel('Price in $s (log scaled)', size=18)\n",
    "ax[1][1].set_xscale('log')\n",
    "\n",
    "# Add median price annotation to boxplot\n",
    "ax[1][1].axvline(median_price, color='lightcoral', linestyle='--', label=f'Median: ${median_price:,.0f}')\n",
    "ax[1][1].legend()\n",
    "\n",
    "# Adjust layout and display the plot\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225b49db-8334-4508-a5b5-5247fea1e654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'data' is the DataFrame containing your dataset\n",
    "def filter_correlations(corr_matrix, threshold=0.5):\n",
    "    mask = np.abs(corr_matrix) >= threshold\n",
    "    filtered_corr = corr_matrix.where(mask).fillna(0)\n",
    "    return filtered_corr\n",
    "\n",
    "def plot_heatmap(matrix, title, size=(14, 10)):\n",
    "    mask = np.triu(np.ones_like(matrix, dtype=bool))  # Create a mask for the upper triangle\n",
    "    plt.figure(figsize=size)\n",
    "    sns.heatmap(matrix, mask=mask, annot=True, cmap='coolwarm', fmt=\".2f\", annot_kws={\"size\": 10})\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "# Select only numeric columns from the DataFrame\n",
    "numeric_cols = df.select_dtypes(include=['number'])\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr_matrix = numeric_cols.corr()\n",
    "\n",
    "# Filter the correlation matrix to only show high correlations\n",
    "filtered_corr_matrix = filter_correlations(corr_matrix, threshold=0)\n",
    "\n",
    "# Plot heatmap with the upper triangle masked\n",
    "plot_heatmap(filtered_corr_matrix, 'Filtered Correlation Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a547e2-61e6-41ca-9dac-60fdfd741f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract correlations for \"price\"\n",
    "price_corr = corr_matrix['price']\n",
    "\n",
    "# Sort the correlations by absolute value (distance from 0) in descending order\n",
    "# While keeping the original sign of the correlation\n",
    "sorted_price_corr = price_corr.iloc[(-price_corr.abs()).argsort()]\n",
    "\n",
    "# Format the output for better readability\n",
    "print(\"Features sorted by strength of correlation with price (absolute value):\")\n",
    "print(\"------------------------------------------------------\")\n",
    "print(f\"{'Feature':<25} {'Correlation':>10}\")\n",
    "print(\"-\" * 36)\n",
    "for feature, corr in sorted_price_corr.items():\n",
    "    print(f\"{feature:<25} {corr:>10.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536a3cf6-2073-42c2-9be0-86ecfcaa21a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scatter_subplots(df):\n",
    "    numerical_features = ['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors',\n",
    "                          'waterfront', 'view', 'condition', 'grade', 'sqft_above',\n",
    "                          'sqft_basement', 'yr_built', 'yr_renovated', 'lat', 'long',\n",
    "                          'sqft_living15', 'sqft_lot15']\n",
    "    \n",
    "    num_plots = len(numerical_features)\n",
    "    rows = (num_plots // 3) + 1\n",
    "    fig, axes = plt.subplots(rows, 3, figsize=(18, 5 * rows))  # Slightly wider figure\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, feature in enumerate(numerical_features):\n",
    "        # Create scatter plot\n",
    "        sns.scatterplot(x=df[feature], y=df['price'], alpha=0.5, ax=axes[i])\n",
    "        \n",
    "        # Add regression line\n",
    "        sns.regplot(x=df[feature], y=df['price'], scatter=False, \n",
    "                    color='red', line_kws={'linewidth': 2}, ax=axes[i])\n",
    "        \n",
    "        # Calculate correlation coefficient\n",
    "        corr = df[feature].corr(df['price'])\n",
    "        \n",
    "        # Format title with correlation\n",
    "        axes[i].set_title(f'Price vs {feature}\\nCorrelation: {corr:.2f}', pad=15)\n",
    "        axes[i].set_xlabel(feature)\n",
    "        axes[i].set_ylabel('Price')\n",
    "        \n",
    "        # Rotate x-axis labels if needed for better readability\n",
    "        if len(df[feature].unique()) > 10:  # For features with many unique values\n",
    "            for tick in axes[i].get_xticklabels():\n",
    "                tick.set_rotation(45)\n",
    "    \n",
    "    # Remove empty subplots\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Call the function with the DataFrame\n",
    "plot_scatter_subplots(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee7609e-9064-46e2-803d-1c71bee3bbba",
   "metadata": {},
   "source": [
    "### Basic Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d176242-e2e2-44cf-b316-4c642ea17b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(df, x='price', \n",
    "                   title='Distribution of House Prices',\n",
    "                   labels={'price': 'Price (USD)'},\n",
    "                   log_y=False)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735a7071-4812-4b9b-834d-4bdcf0128146",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(df, x='sqft_living', y='price', \n",
    "                 title='Price vs. Living Area Square Footage',\n",
    "                 labels={'sqft_living': 'Living Area (sqft)', 'price': 'Price (USD)'},\n",
    "                 trendline='ols',\n",
    "                 opacity=0.4)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40dca67e-f460-4e5a-a07a-8976f9dabda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the histogram\n",
    "fig = px.histogram(df, x='yr_built', \n",
    "                   title='Distribution of Houses Built Per Year', \n",
    "                   labels={'yr_built': 'Year Built'})\n",
    "\n",
    "# Customize axis labels\n",
    "fig.update_layout(\n",
    "    xaxis_title='Year Built',\n",
    "    yaxis_title='Number of Houses'\n",
    ")\n",
    "\n",
    "# Show the figure\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ea066d-802c-47fb-b052-5b770c183131",
   "metadata": {},
   "outputs": [],
   "source": [
    "year_price = df.groupby('yr_built')['price'].mean().reset_index()\n",
    "fig = px.line(year_price, x='yr_built', y='price', \n",
    "              title='Average Price Trend by Year Built',\n",
    "              labels={'yr_built': 'Year Built', 'price': 'Average Price (USD)'})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4992b14-89ab-417f-a953-a2fcaa97514a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.violin(df, x='waterfront', y='price', \n",
    "                title='Price Distribution: Waterfront vs Non-Waterfront',\n",
    "                labels={'waterfront': 'Waterfront Property', 'price': 'Price (USD)'},\n",
    "                box=True)\n",
    "fig.update_layout(xaxis=dict(tickvals=[0, 1], ticktext=['No', 'Yes']))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8945b03-8132-4cf1-b9d3-bd52aee16448",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['price_per_sqft'] = df['price'] / df['sqft_living']\n",
    "fig = px.treemap(df, path=['zipcode'], values='price_per_sqft',\n",
    "                 color='price_per_sqft', color_continuous_scale='RdYlGn',\n",
    "                 title='Price per Sqft by Zipcode')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a039efda-87ad-49a7-a9c2-906af9dde31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "renovated = df[df['yr_renovated'] > 0].copy()\n",
    "renovated['decade_renovated'] = (renovated['yr_renovated'] // 10) * 10\n",
    "fig = px.scatter(renovated, x='decade_renovated', y='price', \n",
    "                 size='sqft_living', color='grade',\n",
    "                 title='Renovation Impact Over Decades',\n",
    "                 labels={'decade_renovated': 'Decade Renovated'})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926fa8f0-f954-429a-abcd-b5c7fdf8e4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(df, x='yr_built', y='price', \n",
    "                   histfunc='avg',\n",
    "                   title='Average Price by Year Built',\n",
    "                   labels={'yr_built': 'Year Built'})\n",
    "fig.add_scatter(x=renovated['yr_renovated'], y=renovated['price'],\n",
    "                 mode='markers', name='Renovations',\n",
    "                 marker=dict(color='red', size=4))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295674f4-46c4-4d00-bebd-69b59dec784c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the DataFrame by 'decade_built' to ensure proper order\n",
    "df['decade_built'] = (df['yr_built'] // 10) * 10\n",
    "df = df.sort_values(by='decade_built')\n",
    "\n",
    "# Create the scatter plot\n",
    "fig = px.scatter(df, x='sqft_living', y='price', \n",
    "                  animation_frame='decade_built', \n",
    "                  size='sqft_lot', color='grade',\n",
    "                  range_x=[0, 10000], range_y=[0, 4e6],\n",
    "                  title='Evolution of Housing Characteristics Over Decades')\n",
    "\n",
    "# Adjust animation speed\n",
    "fig.layout.updatemenus[0].buttons[0].args[1]['frame']['duration'] = 1000\n",
    "\n",
    "# Show the figure\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cf0549-e931-47af-a903-e8ae132029e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(df, x='sqft_living', y='price',\n",
    "                  facet_col='condition', facet_col_wrap=3,\n",
    "                  color='grade', trendline='ols',\n",
    "                  title='Price vs Living Area by Property Condition',\n",
    "                  height=900)\n",
    "fig.for_each_annotation(lambda a: a.update(text=a.text.split(\"=\")[1]))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52f941d-8512-452a-ac77-10e735e72f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chart 1: Bathrooms vs. Price\n",
    "fig = px.box(df, x='bathrooms', y='price',\n",
    "              title='Price Distribution by Number of Bathrooms',\n",
    "              labels={'bathrooms': 'Number of Bathrooms', 'price': 'Price (USD)'})\n",
    "fig.update_layout(yaxis_range=[0, 3e6])\n",
    "fig.show()\n",
    "\n",
    "# Chart 2: Bedrooms vs. Price\n",
    "fig = px.box(df, x='bedrooms', y='price',\n",
    "              title='Price Distribution by Number of Bedrooms',\n",
    "              labels={'bedrooms': 'Number of Bedrooms', 'price': 'Price (USD)'})\n",
    "fig.update_layout(yaxis_range=[0, 3e6])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7746fa-8b05-41d8-8522-a78ae3495883",
   "metadata": {},
   "source": [
    "### Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93c7dfa-0ca2-4b59-a8c5-23647fc3fd08",
   "metadata": {},
   "source": [
    "We will use the variables to predict the prices of properties and see the accuracy of different ML models (learned from Intro to Data Science). \n",
    "\n",
    "We will use four different models:\n",
    "- Linear Regression\n",
    "- Random Forest Regressor\n",
    "- K-Nearest Neighbors Regressor\n",
    "- Naive Bayes Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3b91fe-31af-4dec-90c3-4813b7a205b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df is your DataFrame\n",
    "X = df[['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors', 'waterfront',\n",
    "        'view', 'condition', 'grade', 'sqft_above', 'sqft_basement', 'yr_built',\n",
    "        'lat', 'long', 'sqft_living15', 'sqft_lot15']]\n",
    "y = df['price']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize selected models\n",
    "models = {\n",
    "    'Linear Regression': make_pipeline(StandardScaler(), LinearRegression()),\n",
    "    'Random Forest Regressor': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    'K-Nearest Neighbors Regressor': KNeighborsRegressor(n_neighbors=5),\n",
    "    'Naive Bayes Regressor': GaussianNB()  # Note: GaussianNB is actually a classifier, not a regressor\n",
    "}\n",
    "\n",
    "# Dictionary to store results\n",
    "results = {}\n",
    "\n",
    "# Train each model, make predictions, and evaluate performance\n",
    "for model_name, model in models.items():\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    # Store results\n",
    "    results[model_name] = {'MAE': mae, 'MSE': mse, 'R2': r2}\n",
    "\n",
    "# Convert results into a DataFrame\n",
    "results_df = pd.DataFrame(results).T\n",
    "\n",
    "# Display the comparison of models\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b0da3c-a69a-4c3b-b012-7f8605abf9a4",
   "metadata": {},
   "source": [
    "- Mean Absolute Error (MAE): Measures the average absolute difference between predicted and actual values. It gives an idea of how far off predictions are, without considering direction.\n",
    "- Mean Squared Error (MSE): Similar to MAE but squares the errors before averaging them. This penalizes larger errors more, making it useful when you want to emphasize big mistakes.\n",
    "- R-Squared (R²): Also known as the coefficient of determination, it indicates how well the model explains the variance in the data. A value closer to 1 means better predictive power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08236a91-b2a5-4047-9e39-f00067a7c41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Linear Regression model\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train, y_train)  # y_train is original continuous values\n",
    "\n",
    "# Get continuous predictions on full dataset\n",
    "y_full_cont_pred = lin_reg.predict(X)\n",
    "\n",
    "# Discretize the predictions using the same quantile bins as the original\n",
    "y_full_pred_cat = pd.qcut(y_full_cont_pred, q=3, labels=class_labels)\n",
    "\n",
    "# Generate confusion matrix\n",
    "cm = confusion_matrix(y_full_cat, y_full_pred_cat, labels=class_labels)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "disp.plot(ax=ax, cmap='Blues', colorbar=False)\n",
    "ax.set_title('Linear Regression (Discretized Predictions)', pad=20)\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report for Linear Regression (Full Dataset):\")\n",
    "print(classification_report(y_full_cat, y_full_pred_cat, target_names=class_labels))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ea7ac4-9fee-47a7-b2f2-33b49ccee38b",
   "metadata": {},
   "source": [
    "#### Let's take a deep dive into the classification report\n",
    "\n",
    "- Precision: The proportion of true positive predictions relative to all positive predictions made.\n",
    "- Recall: The proportion of actual positive instances that were correctly identified.\n",
    "- F1-Score: A harmonic mean of precision and recall, providing a single metric to evaluate the balance between them.\n",
    "- Support: The count of actual instances in each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c052e6-a3c1-46e4-ad1b-09f3c54a2e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define class order (High, Medium, Low)\n",
    "class_labels = [\"High\", \"Medium\", \"Low\"]\n",
    "\n",
    "# Convert ALL prices to categories (not just train/test)\n",
    "y_full_cat = pd.qcut(y, q=3, labels=class_labels)  # Apply to entire y\n",
    "\n",
    "# Initialize models\n",
    "class_models = {\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'K-Neighbors': KNeighborsClassifier(n_neighbors=5),\n",
    "    'Naive Bayes': GaussianNB()\n",
    "}\n",
    "\n",
    "# Create 2x2 grid\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "# Train on training set, predict on FULL dataset\n",
    "for idx, (model_name, model) in enumerate(class_models.items()):\n",
    "    model.fit(X_train, y_train_cat)  # Train on training data\n",
    "    y_full_pred = model.predict(X)    # Predict on ALL data (X instead of X_test)\n",
    "    \n",
    "    # Generate confusion matrix (full dataset)\n",
    "    cm = confusion_matrix(y_full_cat, y_full_pred, labels=class_labels)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels)\n",
    "    \n",
    "    # Plot\n",
    "    disp.plot(ax=axes[idx], cmap='Blues', colorbar=False)\n",
    "    axes[idx].set_title(f'{model_name} (Full Data)', pad=20)\n",
    "    \n",
    "    # Print classification report (full dataset)\n",
    "    print(f\"\\nClassification Report for {model_name} (Full Dataset):\")\n",
    "    print(classification_report(y_full_cat, y_full_pred, target_names=class_labels))\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "# Hide empty subplot\n",
    "axes[3].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c22785-12f1-40cd-9450-db27a7a71aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 2x2 grid\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "fig.suptitle('Comparison of Model Evaluation Metrics', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Identify the best and worst models for each metric (kept for reference but not used in coloring)\n",
    "best_mae_model = results_df['MAE'].idxmin()\n",
    "worst_mae_model = results_df['MAE'].idxmax()\n",
    "\n",
    "best_mse_model = results_df['MSE'].idxmin()\n",
    "worst_mse_model = results_df['MSE'].idxmax()\n",
    "\n",
    "best_r2_model = results_df['R2'].idxmax()\n",
    "worst_r2_model = results_df['R2'].idxmin()\n",
    "\n",
    "# Print the best and worst models with their respective values\n",
    "print(f\"Best MAE Model: {best_mae_model} (MAE: {results_df.loc[best_mae_model, 'MAE']:.4f})\")\n",
    "print(f\"Worst MAE Model: {worst_mae_model} (MAE: {results_df.loc[worst_mae_model, 'MAE']:.4f})\\n\")\n",
    "\n",
    "print(f\"Best MSE Model: {best_mse_model} (MSE: {results_df.loc[best_mse_model, 'MSE']:.4f})\")\n",
    "print(f\"Worst MSE Model: {worst_mse_model} (MSE: {results_df.loc[worst_mse_model, 'MSE']:.4f})\\n\")\n",
    "\n",
    "print(f\"Best R² Model: {best_r2_model} (R²: {results_df.loc[best_r2_model, 'R2']:.4f})\")\n",
    "print(f\"Worst R² Model: {worst_r2_model} (R²: {results_df.loc[worst_r2_model, 'R2']:.4f})\\n\")\n",
    "\n",
    "# Plot MAE with uniform color\n",
    "axes[0, 0].bar(results_df.index, results_df['MAE'], color='skyblue', edgecolor='black')\n",
    "axes[0, 0].set_title('Mean Absolute Error (MAE)', fontsize=14)\n",
    "axes[0, 0].set_ylabel('MAE Score')\n",
    "axes[0, 0].set_xticks(range(len(results_df.index)))\n",
    "axes[0, 0].set_xticklabels(results_df.index, rotation=45, ha='right', fontsize=10)\n",
    "axes[0, 0].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Plot MSE with uniform color\n",
    "axes[0, 1].bar(results_df.index, results_df['MSE'], color='lightcoral', edgecolor='black')\n",
    "axes[0, 1].set_title('Mean Squared Error (MSE)', fontsize=14)\n",
    "axes[0, 1].set_ylabel('MSE Score')\n",
    "axes[0, 1].set_xticks(range(len(results_df.index)))\n",
    "axes[0, 1].set_xticklabels(results_df.index, rotation=45, ha='right', fontsize=10)\n",
    "axes[0, 1].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Plot R² with uniform color\n",
    "axes[1, 0].bar(results_df.index, results_df['R2'], color='limegreen', edgecolor='black')\n",
    "axes[1, 0].set_title('R² Score', fontsize=14)\n",
    "axes[1, 0].set_ylabel('R² Value')\n",
    "axes[1, 0].set_xticks(range(len(results_df.index)))\n",
    "axes[1, 0].set_xticklabels(results_df.index, rotation=45, ha='right', fontsize=10)\n",
    "axes[1, 0].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Leave the bottom-right slot blank\n",
    "axes[1, 1].axis('off')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "\n",
    "# Show the grid\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c067cdc-410d-472c-ab23-62fdc9c5fbde",
   "metadata": {},
   "source": [
    "Linear regression and random forest regressors tend to perform better than K-Nearest Neighbors (KNN) and Naive Bayes for housing price prediction datasets due to their inherent strengths in handling structured, numerical data with complex relationships. Housing datasets typically contain a mix of numerical features like square footage and number of bedrooms, along with categorical variables like neighborhood or property type. Linear regression excels in this scenario because it effectively models the linear relationships between these features and the target price variable. It provides interpretable coefficients that show how each feature contributes to the price, such as quantifying how much an additional bedroom increases a home's value. The algorithm also handles continuous numerical data naturally without requiring extensive preprocessing, making it well-suited for housing market analysis where precise price predictions are crucial.\n",
    "\n",
    "Random forest regressors outperform other models because they automatically capture both linear and non-linear relationships in the data through their ensemble decision tree approach. Housing prices are influenced by complex interactions between features—for example, a large backyard may significantly increase value in suburban areas but matter less in urban settings. Random forests naturally model these interactions without explicit feature engineering. They also handle mixed data types well, are robust to outliers (like unusually large or small properties), and provide feature importance scores that reveal which factors most impact pricing. Additionally, their ability to process high-dimensional data makes them ideal for datasets with numerous features like those found in real estate.\n",
    "\n",
    "KNN struggles with housing data primarily due to the \"curse of dimensionality.\" As housing datasets often contain 10-20 features (square footage, bedrooms, location coordinates, etc.), the concept of distance becomes less meaningful in such high-dimensional spaces. This makes it difficult for KNN to find truly comparable neighboring properties. The algorithm also requires all features to be carefully scaled, as it weighs bedroom count equally with square footage by default, which doesn't reflect real-world pricing dynamics. Moreover, KNN's computational inefficiency becomes problematic with large housing datasets, as it must compare each property against all others during prediction. While KNN might work decently for location-based pricing using just latitude and longitude, it generally underperforms for comprehensive housing price prediction.\n",
    "\n",
    "Naive Bayes is poorly suited for housing price prediction because its fundamental assumption of feature independence rarely holds in real estate data. In practice, housing features are highly interdependent—the number of bedrooms correlates with square footage, which in turn relates to neighborhood demographics. The algorithm also struggles with continuous numerical variables, requiring artificial discretization that loses valuable pricing information. Unlike regression models that can output precise dollar amounts, Naive Bayes predicts probability distributions, making it ill-equipped for exact price estimation. While it could potentially work for categorical predictions like \"above/below median price,\" even this approach would be outperformed by random forests or logistic regression due to Naive Bayes' inability to model feature interactions properly. For these reasons, it typically ranks as the weakest choice among the four algorithms for housing price prediction tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4911966-f855-46a0-9b9f-2f41a0480d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with subplots\n",
    "fig, ax = plt.subplots(1, 3, gridspec_kw={'width_ratios': [6, 3, 3]})\n",
    "fig.set_size_inches(18, 6)  # Increased figure size for better visibility\n",
    "fig.suptitle('Training & Test Data Overview', size=21, y=1.05)\n",
    "\n",
    "# Calculate medians\n",
    "train_median = np.median(y_train)\n",
    "test_median = np.median(y_test)\n",
    "\n",
    "# Scatter plot for training and test data\n",
    "s1 = ax[0].scatter(np.arange(len(y_train)), y_train, marker='.', color='dodgerblue', alpha=0.7, label='Train')\n",
    "s2 = ax[0].scatter(np.linspace(0, len(y_train), len(y_test)), y_test, marker='.', color='red', alpha=0.7, label='Test')\n",
    "\n",
    "# Add median lines with labels\n",
    "ax[0].axhline(train_median, color='dodgerblue', linestyle='--', linewidth=2, \n",
    "              label=f'Train Median: ${train_median:,.0f}')\n",
    "ax[0].axhline(test_median, color='red', linestyle='--', linewidth=2, \n",
    "              label=f'Test Median: ${test_median:,.0f}')\n",
    "\n",
    "ax[0].legend(loc='upper left', fontsize=10)\n",
    "ax[0].set_ylabel('Latest Price (in $)', size=15)\n",
    "ax[0].set_xlabel('Homes', size=15)\n",
    "ax[0].set_yscale('log')\n",
    "ax[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Define intervals for price ranges\n",
    "labels = ['$0 - $250K', '$250K - $400K', '$400K - $600K', '$600K - $800K', '$800K - $1.5M', 'More than $1.5M']\n",
    "colors = ['#13af2a', '#10d32e', '#dbf70c', '#f4ac04', '#e86914', '#f41313']\n",
    "intervals = [0, 250000, 400000, 600000, 800000, 1500000, max(y_train.max(), y_test.max())]\n",
    "\n",
    "# Pie chart (Train set)\n",
    "train_data = pd.DataFrame({'price': y_train})\n",
    "chart_slice_sizes = train_data.groupby(pd.cut(train_data['price'], intervals)).size().values\n",
    "ax[1].set_title(f'Train Categories\\n(Median: ${train_median:,.0f})', size=15, pad=20)\n",
    "wedges, texts, autotexts = ax[1].pie(chart_slice_sizes, labels=labels, colors=colors, \n",
    "                                     startangle=30, autopct='%1.1f%%', wedgeprops={'edgecolor': 'black'})\n",
    "plt.setp(autotexts, size=10, weight=\"bold\")  # Make percentages more readable\n",
    "\n",
    "# Pie chart (Test set)\n",
    "test_data = pd.DataFrame({'price': y_test})\n",
    "chart_slice_sizes = test_data.groupby(pd.cut(test_data['price'], intervals)).size().values\n",
    "ax[2].set_title(f'Test Categories\\n(Median: ${test_median:,.0f})', size=15, pad=20)\n",
    "wedges, texts, autotexts = ax[2].pie(chart_slice_sizes, labels=labels, colors=colors, \n",
    "                                    startangle=30, autopct='%1.1f%%', wedgeprops={'edgecolor': 'black'})\n",
    "plt.setp(autotexts, size=10, weight=\"bold\")  # Make percentages more readable\n",
    "\n",
    "# Add median comparison annotation\n",
    "fig.text(0.5, -0.05, \n",
    "         f\"Median Price Comparison: Train (${train_median:,.0f}) vs Test (${test_median:,.0f}) | Difference: ${abs(train_median-test_median):,.0f} ({abs(train_median-test_median)/train_median*100:.1f}%)\",\n",
    "         ha='center', fontsize=12, bbox=dict(facecolor='lightgray', alpha=0.5))\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.subplots_adjust(top=0.85)  # Adjust top spacing for main title\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcdbc52-9bd4-4ffc-a45d-8c2f661539c2",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32774019-837c-471f-bae9-571be59c443f",
   "metadata": {},
   "source": [
    "The King County housing dataset reveals significant trends in property values, driven by location, size, and amenities. Prices range widely from $75,000$ to $7.7$ million, with a median of $450,000, reflecting a diverse market. Waterfront properties and renovated homes command substantial premiums, particularly in affluent neighborhoods like Mercer Island and Queen Anne. Geographic coordinates (lat and long) highlight clustering of high-value properties near Seattle’s urban core and waterfronts, while suburban areas such as Auburn and Kent feature lower prices but larger lot sizes. Square footage emerges as a critical factor, with larger homes (averaging 2,080 sqft of living space) correlating strongly with higher prices. Most properties have 3 bedrooms and 2 bathrooms, though outliers like homes with 33 bedrooms suggest potential data anomalies. Renovations, while rare (only 3.9% of homes), boost value, particularly when paired with modern amenities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d054864-c5ed-47b4-a5c1-e789f2cc5055",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b23778-ea91-46d0-a360-42902cf117c9",
   "metadata": {},
   "source": [
    "Location, size, and amenities are the primary drivers of housing prices in King County. Proximity to urban centers like Seattle and Bellevue, as well as waterfront access, significantly elevates property values. Square footage, particularly living space and above-ground area, is the strongest predictor of price, underscoring buyer preference for spacious homes. Waterfront status and high construction grades further amplify premiums, reflecting demand for luxury and quality. However, affordability challenges persist in high-demand urban zones, pushing buyers toward suburban areas where larger lots are more accessible. The dataset also hints at broader societal trends, such as growing emphasis on sustainability through renovations and energy-efficient designs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
